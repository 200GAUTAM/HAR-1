# Pattern Classification config file

# Details relating to each of the functions mentioned in this 
# file are explained in the README file.

#------------ Input Data Information ----------------------------#
# Divide the dataset into 3 sets-
# 1. Training set
# 2. Test Set Intermediate
# 3. Final Test Set
# In cases where you dont have a separate Final Test, enter the same path
# for it as the Test Set.
# Final test set used to evaluate your final classification performance.
# Keep the features and labels into 2 different files
# Features in the files are assumed to comma separated else 
# make appropriate changes in the read function.

# Enter the Train data file
[data]
traindataFile = ../../Project/UCI HAR Dataset/train/X_train.txt
trainLabelFile = ../../Project/UCI HAR Dataset/train/y_train.txt
# Enter the Test intermediate data file
testdataFile = ../../Project/UCI HAR Dataset/test/X_test.txt
testLabelFile = ../../Project/UCI HAR Dataset/test/y_test.txt
# Enter the Test final data file
testFDataFile = ../../Project/UCI HAR Dataset/test/X_test.txt
testFLabelFile = ../../Project/UCI HAR Dataset/test/y_test.txt

#---------------- Data Normalization ----------- #
# This section is used to normalize the data either by mean or variance
[Data_norm]
useThis = FALSE
# Use any of [mean, variance,domain]
routine = domain

#--------- Dimension Reduction/ Feature Selection -------------------#

# Use either Dimension Reduction technique or Feature Selection
# Do NOT use both

# **** Dimension Reduction ***** #
[Dim_Red]
# Use any of [pca, klm, fisherm]
useThis = TRUE
routine = pca
# Number of dimensions to reduce to
k = 100

# **** Feature Selection ****** #
# NOTE: This routine takes a lot of time to run
[Feat_sel]
useThis = FALSE
# Use any of [featself, featseli, featselb,cmap]
routine = cmap
# Use any of the criterion ['in-in','maha-s','maha-m','eucl-s','eucl-m'
# 'NN','mad','mse, ldc, qdc, perlc...(any of the criterion)]
crit = ldc
# Number of features to select
k = 2
# Number of cross validations
N = 3

# ------------ Classification ------------------------------------ #

# *********** Distribution Free Classifier ****************** #
[Dist_free]
useThis = TRUE
# Use any of the [fisherc, perlc, nmc, nmsc, polyc]
routine = fisherc
# For Polynomial classification only
# Enter the degree of the polynomial
d = 1
# Use cross terms s - 0/1
s = 0

#Cross Validation
doCrossValidation = TRUE
# n -fold cross val
n_folds = 3
# Number of times
n_times = 1

no_of_iteations = 100

# ********************* SVM *********************************** #

[SVM]
useThis = FALSE
#'polynomial'   | 'p': SIGN(A*B'+1).*(A*B'+1).^P
#'homogeneous'  | 'h': SIGN(A*B').*(A*B').^P
#'exponential'  | 'e': EXP(-(||A-B||)/P)
#'radial_basis' | 'r': EXP(-(||A-B||.^2)/(P*P))
#'sigmoid'      | 's': SIGM((SIGN(A*B').*(A*B'))/P)
#'distance'     | 'd': ||A-B||.^P
#'minkowski'    | 'm': SUM(|A-B|^P).^(1/P)
#'city-block'   | 'c': SUM(|A-B|)
#'cosine'       | 'o': 1 - (A*B')/||A||*||B|| 
# Use any of the [p,h,e,r,s,d,m,c,o]
routine = r
#Enter the degree/ Value of P in the above expressions
Deg = 2
# Enter the Trade_off parameter
C = 1000

#Cross Validation
doCrossValidation = FALSE
# n -fold cross val
n_folds = 3
# Number of times
n_times = 3

# ------------------- Statistical classifier --------------------------- #

# ___ Parametric Classifier ___ #
[Param]
useThis = FALSE
# Use any of the [ldc, qdc, udc]
routine = udc
#Cross Validation
doCrossValidation = FALSE
# n -fold cross val
n_folds = 3
# Number of times
n_times = 1

no_of_iterations = 100

# ___ Non-Parametric Classifier ___ #
[Non_param]
useThis = FALSE
# Use any of the [knnc, parzenc]
routine = parzenc
# For knnc, enter the value of k
# Use k = 0 for the program to select the optimum value for you
k = 0
#Cross Validation
doCrossValidation = FALSE
# n -fold cross val
n_folds = 3
# Number of times
n_times = 3

# ----------------------------------------------------------#

# Weight function will be stores in W
# Error will be stored in Err
# Results will be evaluated for test set intermediate
# Once you finalize the classifier, you can use the weight function
# and test it on test set final.
